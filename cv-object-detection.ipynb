{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":2674232,"sourceType":"datasetVersion","datasetId":1627144},{"sourceId":497494,"sourceType":"datasetVersion","datasetId":233357}],"dockerImageVersionId":30775,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/aleksandrmorozov123/cv-object-detection?scriptVersionId=198563926\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Crowd counting**","metadata":{}},{"cell_type":"code","source":"import torch\nimport os\nimport h5py\nfrom scipy import io\n\nif not os.path.exists('CSRNet-pytorch/'):\n    %pip install -U scipy torch_snippets torch_summary\n    !git clone https://github.com/sizhky/CSRNet-pytorch.git\n\n%cd CSRNet-pytorch","metadata":{"execution":{"iopub.status.busy":"2024-09-27T18:46:29.044871Z","iopub.execute_input":"2024-09-27T18:46:29.045616Z","iopub.status.idle":"2024-09-27T18:46:53.697277Z","shell.execute_reply.started":"2024-09-27T18:46:29.045581Z","shell.execute_reply":"2024-09-27T18:46:53.696166Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Requirement already satisfied: scipy in /opt/conda/lib/python3.10/site-packages (1.14.1)\nCollecting torch_snippets\n  Downloading torch_snippets-0.545-py3-none-any.whl.metadata (16 kB)\nCollecting torch_summary\n  Downloading torch_summary-1.4.5-py3-none-any.whl.metadata (18 kB)\nRequirement already satisfied: numpy<2.3,>=1.23.5 in /opt/conda/lib/python3.10/site-packages (from scipy) (1.26.4)\nRequirement already satisfied: fastcore in /opt/conda/lib/python3.10/site-packages (from torch_snippets) (1.7.8)\nRequirement already satisfied: matplotlib in /opt/conda/lib/python3.10/site-packages (from torch_snippets) (3.7.5)\nRequirement already satisfied: Pillow in /opt/conda/lib/python3.10/site-packages (from torch_snippets) (10.3.0)\nRequirement already satisfied: dill in /opt/conda/lib/python3.10/site-packages (from torch_snippets) (0.3.8)\nCollecting loguru (from torch_snippets)\n  Downloading loguru-0.7.2-py3-none-any.whl.metadata (23 kB)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from torch_snippets) (2.2.2)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from torch_snippets) (4.66.4)\nRequirement already satisfied: rich in /opt/conda/lib/python3.10/site-packages (from torch_snippets) (13.7.1)\nRequirement already satisfied: PyYAML in /opt/conda/lib/python3.10/site-packages (from torch_snippets) (6.0.2)\nRequirement already satisfied: catalogue in /opt/conda/lib/python3.10/site-packages (from torch_snippets) (2.0.10)\nRequirement already satisfied: confection in /opt/conda/lib/python3.10/site-packages (from torch_snippets) (0.1.4)\nRequirement already satisfied: pydantic in /opt/conda/lib/python3.10/site-packages (from torch_snippets) (2.9.2)\nCollecting typing (from torch_snippets)\n  Downloading typing-3.7.4.3.tar.gz (78 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.6/78.6 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: srsly in /opt/conda/lib/python3.10/site-packages (from torch_snippets) (2.4.8)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch_snippets) (4.12.2)\nRequirement already satisfied: wasabi in /opt/conda/lib/python3.10/site-packages (from torch_snippets) (1.1.2)\nCollecting jsonlines (from torch_snippets)\n  Downloading jsonlines-4.0.0-py3-none-any.whl.metadata (1.6 kB)\nRequirement already satisfied: imgaug>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from torch_snippets) (0.4.0)\nCollecting xmltodict (from torch_snippets)\n  Downloading xmltodict-0.13.0-py2.py3-none-any.whl.metadata (7.7 kB)\nRequirement already satisfied: fuzzywuzzy in /opt/conda/lib/python3.10/site-packages (from torch_snippets) (0.18.0)\nRequirement already satisfied: nltk in /opt/conda/lib/python3.10/site-packages (from torch_snippets) (3.2.4)\nCollecting python-Levenshtein (from torch_snippets)\n  Downloading python_Levenshtein-0.26.0-py3-none-any.whl.metadata (3.7 kB)\nCollecting pre-commit (from torch_snippets)\n  Downloading pre_commit-3.8.0-py2.py3-none-any.whl.metadata (1.3 kB)\nCollecting icecream (from torch_snippets)\n  Downloading icecream-2.1.3-py2.py3-none-any.whl.metadata (1.4 kB)\nCollecting mergedeep (from torch_snippets)\n  Downloading mergedeep-1.3.4-py3-none-any.whl.metadata (4.3 kB)\nRequirement already satisfied: deepdiff in /opt/conda/lib/python3.10/site-packages (from torch_snippets) (8.0.1)\nRequirement already satisfied: typer in /opt/conda/lib/python3.10/site-packages (from torch_snippets) (0.12.3)\nRequirement already satisfied: six in /opt/conda/lib/python3.10/site-packages (from imgaug>=0.4.0->torch_snippets) (1.16.0)\nRequirement already satisfied: scikit-image>=0.14.2 in /opt/conda/lib/python3.10/site-packages (from imgaug>=0.4.0->torch_snippets) (0.23.2)\nRequirement already satisfied: opencv-python in /opt/conda/lib/python3.10/site-packages (from imgaug>=0.4.0->torch_snippets) (4.10.0.84)\nRequirement already satisfied: imageio in /opt/conda/lib/python3.10/site-packages (from imgaug>=0.4.0->torch_snippets) (2.34.1)\nRequirement already satisfied: Shapely in /opt/conda/lib/python3.10/site-packages (from imgaug>=0.4.0->torch_snippets) (1.8.5.post1)\nRequirement already satisfied: annotated-types>=0.6.0 in /opt/conda/lib/python3.10/site-packages (from pydantic->torch_snippets) (0.7.0)\nRequirement already satisfied: pydantic-core==2.23.4 in /opt/conda/lib/python3.10/site-packages (from pydantic->torch_snippets) (2.23.4)\nRequirement already satisfied: orderly-set==5.2.2 in /opt/conda/lib/python3.10/site-packages (from deepdiff->torch_snippets) (5.2.2)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from fastcore->torch_snippets) (21.3)\nRequirement already satisfied: colorama>=0.3.9 in /opt/conda/lib/python3.10/site-packages (from icecream->torch_snippets) (0.4.6)\nRequirement already satisfied: pygments>=2.2.0 in /opt/conda/lib/python3.10/site-packages (from icecream->torch_snippets) (2.18.0)\nRequirement already satisfied: executing>=0.3.1 in /opt/conda/lib/python3.10/site-packages (from icecream->torch_snippets) (2.0.1)\nRequirement already satisfied: asttokens>=2.0.1 in /opt/conda/lib/python3.10/site-packages (from icecream->torch_snippets) (2.4.1)\nRequirement already satisfied: attrs>=19.2.0 in /opt/conda/lib/python3.10/site-packages (from jsonlines->torch_snippets) (23.2.0)\nRequirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->torch_snippets) (1.2.1)\nRequirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.10/site-packages (from matplotlib->torch_snippets) (0.12.1)\nRequirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib->torch_snippets) (4.53.0)\nRequirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->torch_snippets) (1.4.5)\nRequirement already satisfied: pyparsing>=2.3.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->torch_snippets) (3.1.2)\nRequirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.10/site-packages (from matplotlib->torch_snippets) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->torch_snippets) (2024.1)\nRequirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas->torch_snippets) (2024.1)\nCollecting cfgv>=2.0.0 (from pre-commit->torch_snippets)\n  Downloading cfgv-3.4.0-py2.py3-none-any.whl.metadata (8.5 kB)\nCollecting identify>=1.0.0 (from pre-commit->torch_snippets)\n  Downloading identify-2.6.1-py2.py3-none-any.whl.metadata (4.4 kB)\nCollecting nodeenv>=0.11.1 (from pre-commit->torch_snippets)\n  Downloading nodeenv-1.9.1-py2.py3-none-any.whl.metadata (21 kB)\nRequirement already satisfied: virtualenv>=20.10.0 in /opt/conda/lib/python3.10/site-packages (from pre-commit->torch_snippets) (20.21.0)\nCollecting Levenshtein==0.26.0 (from python-Levenshtein->torch_snippets)\n  Downloading levenshtein-0.26.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.2 kB)\nCollecting rapidfuzz<4.0.0,>=3.9.0 (from Levenshtein==0.26.0->python-Levenshtein->torch_snippets)\n  Downloading rapidfuzz-3.10.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\nRequirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/lib/python3.10/site-packages (from rich->torch_snippets) (3.0.0)\nRequirement already satisfied: click>=8.0.0 in /opt/conda/lib/python3.10/site-packages (from typer->torch_snippets) (8.1.7)\nRequirement already satisfied: shellingham>=1.3.0 in /opt/conda/lib/python3.10/site-packages (from typer->torch_snippets) (1.5.4)\nRequirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich->torch_snippets) (0.1.2)\nRequirement already satisfied: networkx>=2.8 in /opt/conda/lib/python3.10/site-packages (from scikit-image>=0.14.2->imgaug>=0.4.0->torch_snippets) (3.3)\nRequirement already satisfied: tifffile>=2022.8.12 in /opt/conda/lib/python3.10/site-packages (from scikit-image>=0.14.2->imgaug>=0.4.0->torch_snippets) (2024.5.22)\nRequirement already satisfied: lazy-loader>=0.4 in /opt/conda/lib/python3.10/site-packages (from scikit-image>=0.14.2->imgaug>=0.4.0->torch_snippets) (0.4)\nRequirement already satisfied: distlib<1,>=0.3.6 in /opt/conda/lib/python3.10/site-packages (from virtualenv>=20.10.0->pre-commit->torch_snippets) (0.3.8)\nRequirement already satisfied: filelock<4,>=3.4.1 in /opt/conda/lib/python3.10/site-packages (from virtualenv>=20.10.0->pre-commit->torch_snippets) (3.15.1)\nRequirement already satisfied: platformdirs<4,>=2.4 in /opt/conda/lib/python3.10/site-packages (from virtualenv>=20.10.0->pre-commit->torch_snippets) (3.11.0)\nDownloading torch_snippets-0.545-py3-none-any.whl (103 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m103.0/103.0 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading torch_summary-1.4.5-py3-none-any.whl (16 kB)\nDownloading icecream-2.1.3-py2.py3-none-any.whl (8.4 kB)\nDownloading jsonlines-4.0.0-py3-none-any.whl (8.7 kB)\nDownloading loguru-0.7.2-py3-none-any.whl (62 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.5/62.5 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading mergedeep-1.3.4-py3-none-any.whl (6.4 kB)\nDownloading pre_commit-3.8.0-py2.py3-none-any.whl (204 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m204.6/204.6 kB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading python_Levenshtein-0.26.0-py3-none-any.whl (9.4 kB)\nDownloading levenshtein-0.26.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (162 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m162.6/162.6 kB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading xmltodict-0.13.0-py2.py3-none-any.whl (10.0 kB)\nDownloading cfgv-3.4.0-py2.py3-none-any.whl (7.2 kB)\nDownloading identify-2.6.1-py2.py3-none-any.whl (98 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.0/99.0 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading nodeenv-1.9.1-py2.py3-none-any.whl (22 kB)\nDownloading rapidfuzz-3.10.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m69.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hBuilding wheels for collected packages: typing\n  Building wheel for typing (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for typing: filename=typing-3.7.4.3-py3-none-any.whl size=26306 sha256=fd2335c806b56a3b02cdcd8b6c68391739f9b8bf66f46733cc48c056d2fdb60e\n  Stored in directory: /root/.cache/pip/wheels/7c/d0/9e/1f26ebb66d9e1732e4098bc5a6c2d91f6c9a529838f0284890\nSuccessfully built typing\nInstalling collected packages: xmltodict, typing, torch_summary, rapidfuzz, nodeenv, mergedeep, loguru, jsonlines, identify, cfgv, pre-commit, Levenshtein, icecream, python-Levenshtein, torch_snippets\nSuccessfully installed Levenshtein-0.26.0 cfgv-3.4.0 icecream-2.1.3 identify-2.6.1 jsonlines-4.0.0 loguru-0.7.2 mergedeep-1.3.4 nodeenv-1.9.1 pre-commit-3.8.0 python-Levenshtein-0.26.0 rapidfuzz-3.10.0 torch_snippets-0.545 torch_summary-1.4.5 typing-3.7.4.3 xmltodict-0.13.0\nNote: you may need to restart the kernel to use updated packages.\nCloning into 'CSRNet-pytorch'...\nremote: Enumerating objects: 92, done.\u001b[K\nremote: Total 92 (delta 0), reused 0 (delta 0), pack-reused 92 (from 1)\u001b[K\nReceiving objects: 100% (92/92), 623.74 KiB | 17.82 MiB/s, done.\nResolving deltas: 100% (51/51), done.\n/kaggle/working/CSRNet-pytorch\n","output_type":"stream"}]},{"cell_type":"code","source":"from torch_snippets import *\nfrom torchvision import transforms\nfrom torch import optim\nfrom torch_snippets.torch_loader import Report","metadata":{"execution":{"iopub.status.busy":"2024-09-27T19:09:06.622112Z","iopub.execute_input":"2024-09-27T19:09:06.623158Z","iopub.status.idle":"2024-09-27T19:09:09.100497Z","shell.execute_reply.started":"2024-09-27T19:09:06.623112Z","shell.execute_reply":"2024-09-27T19:09:09.099418Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"part_A = '/kaggle/input/shanghaitech-with-people-density-map/ShanghaiTech/part_A/train_data/'\nimage_folder = '/kaggle/input/shanghaitech-with-people-density-map/ShanghaiTech/part_A/train_data/images/'\n\nheatmap_folder = '/kaggle/input/shanghaitech-with-people-density-map/ShanghaiTech/part_A/train_data/ground-truth-h5/'\ngt_folder = '/kaggle/input/shanghaitech-with-people-density-map/ShanghaiTech/part_A/train_data/ground-truth/'","metadata":{"execution":{"iopub.status.busy":"2024-09-27T19:10:27.089225Z","iopub.execute_input":"2024-09-27T19:10:27.090006Z","iopub.status.idle":"2024-09-27T19:10:27.094576Z","shell.execute_reply.started":"2024-09-27T19:10:27.089953Z","shell.execute_reply":"2024-09-27T19:10:27.093653Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"code","source":"from torch.utils.data import Dataset, DataLoader\nimport torch.nn as nn\n\nclass MyDataset(Dataset):\n    def __init__(self,x,y):\n        self.x = torch.tensor(x).float()\n        self.y = torch.tensor(y).float()\n        \n    def __len__(self):\n        return len(self.x)\n    \n    def __getitem__(self, ix):\n        return self.x[ix], self.y[ix]","metadata":{"execution":{"iopub.status.busy":"2024-09-27T18:50:12.883388Z","iopub.execute_input":"2024-09-27T18:50:12.88413Z","iopub.status.idle":"2024-09-27T18:50:12.890786Z","shell.execute_reply.started":"2024-09-27T18:50:12.884089Z","shell.execute_reply":"2024-09-27T18:50:12.889576Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"# define training and validation dataset\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\ntfm = transforms.Compose([transforms.ToTensor()])\n\nclass Crowds(Dataset):\n    def __init__(self, stems):\n        self.stems = stems\n\n    def __len__(self):\n        return len(self.stems)\n\n    def __getitem__(self, ix):\n        _stem = self.stems[ix]\n        image_path = f'{image_folder}/{_stem}.jpg'\n        heatmap_path = f'{heatmap_folder}/{_stem}.h5'\n        gt_path = f'{gt_folder}/GT_{_stem}.mat'\n\n        pts = io.loadmat(gt_path)\n        pts = len(pts['image_info'][0,0][0,0][0])\n\n        image = read(image_path, 1)\n        with h5py.File(heatmap_path, 'r') as hf:\n            gt = hf['density'][:]\n        gt = resize(gt, 1/8)*64\n        return image.copy(), gt.copy(), pts\n\n    def collate_fn(self, batch):\n        ims, gts, pts = list(zip(*batch))\n        ims = torch.cat([tfm(im)[None] for im in ims]).to(device)\n        gts = torch.cat([tfm(gt)[None] for gt in gts]).to(device)\n        return ims, gts, torch.tensor(pts).to(device)\n\n    def choose(self):\n        return self[randint(len(self))]\n\nfrom sklearn.model_selection import train_test_split\ntrn_stems, val_stems = train_test_split(stems(Glob(image_folder)), random_state=10)\n\ntrn_ds = Crowds(trn_stems)\nval_ds = Crowds(val_stems)\n\ntrn_dl = DataLoader(trn_ds, batch_size=1, shuffle=True, collate_fn=trn_ds.collate_fn)\nval_dl = DataLoader(val_ds, batch_size=1, shuffle=True, collate_fn=val_ds.collate_fn)","metadata":{"execution":{"iopub.status.busy":"2024-09-27T19:13:13.594337Z","iopub.execute_input":"2024-09-27T19:13:13.595223Z","iopub.status.idle":"2024-09-27T19:13:13.62937Z","shell.execute_reply.started":"2024-09-27T19:13:13.595181Z","shell.execute_reply":"2024-09-27T19:13:13.628447Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"code","source":"from torchvision import models\nfrom utils import save_net,load_net\n\ndef make_layers(cfg, in_channels = 3,batch_norm=False,dilation = False):\n    if dilation:\n        d_rate = 2\n    else:\n        d_rate = 1\n    layers = []\n    for v in cfg:\n        if v == 'M':\n            layers += [nn.MaxPool2d(kernel_size=2, stride=2)]\n        else:\n            conv2d = nn.Conv2d(in_channels, v, kernel_size=3, padding=d_rate, dilation=d_rate)\n            if batch_norm:\n                layers += [conv2d, nn.BatchNorm2d(v), nn.ReLU(inplace=True)]\n            else:\n                layers += [conv2d, nn.ReLU(inplace=True)]\n            in_channels = v\n    return nn.Sequential(*layers)","metadata":{"execution":{"iopub.status.busy":"2024-09-27T18:50:20.764694Z","iopub.execute_input":"2024-09-27T18:50:20.765756Z","iopub.status.idle":"2024-09-27T18:50:20.774109Z","shell.execute_reply.started":"2024-09-27T18:50:20.765712Z","shell.execute_reply":"2024-09-27T18:50:20.773192Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"# define the network architecture\nclass CSRNet (nn.Module):\n    def __init__ (self, load_weights = False):\n        super (CSRNet, self).__init__ ()\n        self.seen = 0\n        self.frontend_feat = [64, 64, 'M', 128, 128,\n                             'M', 256, 256, 256, 'M', 512, 512, 512]\n        self.backend_feat = [512, 512, 512, 256, 128, 64]\n        self.frontend = make_layers (self.frontend_feat)\n        self.backend = make_layers (self.backend_feat, in_channels = 512, dilation = True)\n        self.output_layer = nn.Conv2d (64, 1, kernel_size = 1)\n        if not load_weights: \n            mod = models.vgg16 (pretrained = True)\n            self._initialize_weights ()\n            items = list (self.frontend.state_dict ().items())\n            _items = list (mod.state_dict ().items())\n            for i in range (len (self.frontend.state_dict().items ())):\n                items[i][1].data[:] = _items[i][1].data[:]\n    def forward (self, x):\n        x = self.frontend (x)\n        x = self.backend (x)\n        x = self.output_layer (x)\n        return x\n    def _initialize_weights (self):\n        for m in self.modules ():\n            if isinstance (m, nn.Conv2d):\n                nn.init.normal_ (m.weight, std = 0.01)\n                if m.bias is not None:\n                    nn.init.constant_ (m.bias, 0)\n            elif isinstance (m, nn.BatchNorm2d):\n                nn.init.constant_ (m.weight, 1)\n                nn.init.constant_ (m.bias, 0)\n    ","metadata":{"execution":{"iopub.status.busy":"2024-09-27T18:57:09.757279Z","iopub.execute_input":"2024-09-27T18:57:09.758074Z","iopub.status.idle":"2024-09-27T18:57:09.769486Z","shell.execute_reply.started":"2024-09-27T18:57:09.758032Z","shell.execute_reply":"2024-09-27T18:57:09.768452Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"# train and validate a batch of data\ndef train_batch (model, data, optimizer, criterion):\n    model.train ()\n    optimizer.zero_grad ()\n    ims, gts, pts = data\n    _gts = model (ims)\n    loss = criterion (_gts, gts)\n    loss.backward ()\n    optimizer.step ()\n    pts_loss = nn.L1Loss ()(_gts.sum(), gts.sum())\n    return loss.item (), pts_loss.item ()\n\n@torch.no_grad()\ndef validate_batch (model, data, criterion):\n    model.eval ()\n    ims, gts, pts = data\n    _gts = model (ims)\n    loss = criterion (_gts, gts)\n    pts_loss = nn.L1Loss ()(_gts.sum(), gts.sum())\n    return loss.item(), pts_loss.item","metadata":{"execution":{"iopub.status.busy":"2024-09-27T18:57:12.521522Z","iopub.execute_input":"2024-09-27T18:57:12.522175Z","iopub.status.idle":"2024-09-27T18:57:12.529676Z","shell.execute_reply.started":"2024-09-27T18:57:12.522133Z","shell.execute_reply":"2024-09-27T18:57:12.528724Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"# train the model\nmodel = CSRNet ().to(device)\ncriterion = nn.MSELoss ()\noptimizer = optim.Adam (model.parameters (), lr = 1e-6)\nn_epochs = 20\n\nlog = Report (n_epochs)\nfor ex in range (n_epochs):\n    N = len (trn_dl)\n    for bx, data in enumerate (trn_dl):\n        loss, pts_loss = train_batch (model, data, optimizer, criterion)\n        log.record (ex+(bx+1)/N, trn_loss = loss, trn_pts_loss = pts_loss, end = '\\r')\n        N = len (val_dl)\n        for bx, data in enumerate (val_dl):\n            loss, pts_loss = validate_batch (model, data, criterion)\n            log.record (ex+(bx+1)/N, val_loss = loss, val_pts_loss = pts_loss, end = '\\r')\n        log.report_avgs (ex + 1)\n        if ex == 10: optimizer = optim.Adam (model.parameters (), lr = 1e-7)","metadata":{"execution":{"iopub.status.busy":"2024-09-27T19:13:19.411548Z","iopub.execute_input":"2024-09-27T19:13:19.412219Z","iopub.status.idle":"2024-09-27T19:13:21.15865Z","shell.execute_reply.started":"2024-09-27T19:13:19.412176Z","shell.execute_reply":"2024-09-27T19:13:21.157047Z"},"trusted":true},"execution_count":34,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","Cell \u001b[0;32mIn[34], line 10\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m ex \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m (n_epochs):\n\u001b[1;32m      9\u001b[0m     N \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m (trn_dl)\n\u001b[0;32m---> 10\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m bx, data \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m (trn_dl):\n\u001b[1;32m     11\u001b[0m         loss, pts_loss \u001b[38;5;241m=\u001b[39m train_batch (model, data, optimizer, criterion)\n\u001b[1;32m     12\u001b[0m         log\u001b[38;5;241m.\u001b[39mrecord (ex\u001b[38;5;241m+\u001b[39m(bx\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m/\u001b[39mN, trn_loss \u001b[38;5;241m=\u001b[39m loss, trn_pts_loss \u001b[38;5;241m=\u001b[39m pts_loss, end \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\r\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    628\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    633\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:673\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    671\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    672\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 673\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    674\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    675\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n","Cell \u001b[0;32mIn[33], line 18\u001b[0m, in \u001b[0;36mCrowds.__getitem__\u001b[0;34m(self, ix)\u001b[0m\n\u001b[1;32m     15\u001b[0m heatmap_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mheatmap_folder\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m_stem\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.h5\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     16\u001b[0m gt_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mgt_folder\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/GT_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m_stem\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.mat\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m---> 18\u001b[0m pts \u001b[38;5;241m=\u001b[39m \u001b[43mio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloadmat\u001b[49m(gt_path)\n\u001b[1;32m     19\u001b[0m pts \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(pts[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimage_info\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m     21\u001b[0m image \u001b[38;5;241m=\u001b[39m read(image_path, \u001b[38;5;241m1\u001b[39m)\n","\u001b[0;31mAttributeError\u001b[0m: 'function' object has no attribute 'loadmat'"],"ename":"AttributeError","evalue":"'function' object has no attribute 'loadmat'","output_type":"error"}]},{"cell_type":"code","source":"# make the inference \nfrom matplotlib import cm as c\nfrom PIL import Image\nfrom torchvision import datasets, transforms\ntransform = transforms.Compose ([\n    transforms.ToTensor (), transforms.Normalize (mean = [0.485, 0.456, 0.406],\n                                                 std = [0.229, 0.224, 0.225]),\n])\ntest_folder = '/kaggle/input/shanghaitech-with-people-density-map/ShanghaiTech/part_A/test_data/'\nimgs = Glob (f'{test_folder}/images')\nf = choose (imgs)\nprint (f)\nimg = transform (Image.open(f).convert('RGB')).to(device)","metadata":{"execution":{"iopub.status.busy":"2024-09-27T18:53:02.914155Z","iopub.execute_input":"2024-09-27T18:53:02.914552Z","iopub.status.idle":"2024-09-27T18:53:03.191654Z","shell.execute_reply.started":"2024-09-27T18:53:02.914508Z","shell.execute_reply":"2024-09-27T18:53:03.190642Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stdout","text":"/kaggle/input/shanghaitech-with-people-density-map/ShanghaiTech/part_A/test_data/images/IMG_126.jpg\n","output_type":"stream"}]},{"cell_type":"code","source":"# pass the image through the trained model\noutput = model (img[None])\nprint (\"Predicted Count: \", int (output.detach().cpu().sum().numpy()))\ntemp = np.asarray (output.detach ().cpu ()\\\n                  .reshape (output.detach().cpu()\\\n                           .shape[2], output.detach()\\\n                           .cpu().shape[3]))\nplt.imshow (temp, cmap = c.jet)\nplt.show ()","metadata":{"execution":{"iopub.status.busy":"2024-09-27T18:54:29.377369Z","iopub.execute_input":"2024-09-27T18:54:29.377824Z","iopub.status.idle":"2024-09-27T18:54:29.417732Z","shell.execute_reply.started":"2024-09-27T18:54:29.377773Z","shell.execute_reply":"2024-09-27T18:54:29.416605Z"},"trusted":true},"execution_count":19,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[19], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# pass the image through the trained model\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m (img[\u001b[38;5;28;01mNone\u001b[39;00m])\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPredicted Count: \u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mint\u001b[39m (output\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39msum()\u001b[38;5;241m.\u001b[39mnumpy()))\n\u001b[1;32m      4\u001b[0m temp \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray (output\u001b[38;5;241m.\u001b[39mdetach ()\u001b[38;5;241m.\u001b[39mcpu ()\\\n\u001b[1;32m      5\u001b[0m                   \u001b[38;5;241m.\u001b[39mreshape (output\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mcpu()\\\n\u001b[1;32m      6\u001b[0m                            \u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m2\u001b[39m], output\u001b[38;5;241m.\u001b[39mdetach()\\\n\u001b[1;32m      7\u001b[0m                            \u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m3\u001b[39m]))\n","\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"],"ename":"NameError","evalue":"name 'model' is not defined","output_type":"error"}]}]}