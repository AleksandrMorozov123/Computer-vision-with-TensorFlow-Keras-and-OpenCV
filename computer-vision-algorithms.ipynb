{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":29606,"sourceType":"datasetVersion","datasetId":23123},{"sourceId":5456605,"sourceType":"datasetVersion","datasetId":2732945},{"sourceId":6664020,"sourceType":"datasetVersion","datasetId":3845355},{"sourceId":7618708,"sourceType":"datasetVersion","datasetId":4437451}],"dockerImageVersionId":30684,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/aleksandrmorozov123/computer-vision-algorithms?scriptVersionId=177664094\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**SIFT** \nThe Scale-Invariant Feature Transform (SIFT) algorithm is a computer vision algorithm used for identifying and matching local features, such as corners or blobs, in images. It was first described in a paper by David Lowe in 1999. The SIFT algorithm is invariant to image scale and rotation. SIFT is widely used in image matching, object recognition and image registration apllications. ","metadata":{}},{"cell_type":"code","source":"# SIFT implementation with OpenCV\nimport cv2\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ntrain_img = cv2.imread ('/kaggle/input/astronomy-picture-of-the-day-data-collection/apod_images/apod_images/2023-03-15.jpg')\nq_img = cv2.imread ('/kaggle/input/astronomy-picture-of-the-day-data-collection/apod_images/apod_images/2023-03-15.jpg')\nquery_img = cv2.rotate(q_img, cv2.ROTATE_90_CLOCKWISE)\n\n# turn images to grayscale\ndef to_gray (color_img):\n    gray = cv2.cvtColor (color_img, cv2.COLOR_BGR2GRAY)\n    return gray\n\ntrain_img_gray = to_gray (train_img)\nquery_img_gray = to_gray (query_img)\n\n# initialize SIFT detector\nsift = cv2.SIFT_create ()\n\n# generate SIFT keypoints and descriptors\ntrain_kp, train_desc = sift.detectAndCompute (train_img_gray, None)\nquery_kp, query_desc = sift.detectAndCompute (query_img_gray, None) \n\nplt.figure (1)\nplt.imshow ((cv2.drawKeypoints (train_img_gray, train_kp, train_img.copy ())))\nplt.title ('Train image keypoints')\n\nplt.figure (2)\nplt.imshow ((cv2.drawKeypoints (query_img_gray, query_kp, query_img.copy ())))\nplt.title ('Query image keypoints')\n\nplt.show ()","metadata":{"execution":{"iopub.status.busy":"2024-04-15T08:25:32.766866Z","iopub.execute_input":"2024-04-15T08:25:32.76732Z","iopub.status.idle":"2024-04-15T08:25:33.947099Z","shell.execute_reply.started":"2024-04-15T08:25:32.767285Z","shell.execute_reply":"2024-04-15T08:25:33.946143Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# create a BFmatcher object which will match up the SIFT features\nbf = cv2.BFMatcher (cv2.NORM_L2, crossCheck = True)\nmatches = bf.match (train_desc, query_desc)\n\n# sort the matches in the order of their distance\nmatches = sorted (matches, key = lambda x:x.distance)\n\n# draw the top N matches\nN_MATCHES = 100\n\nmatch_img = cv2.drawMatches (train_img, train_kp,\n                             query_img, query_kp,\n                             matches [:N_MATCHES], query_img.copy (), flags = 0)\n\nplt.figure (3)\nplt.imshow (match_img)\nplt.show ()","metadata":{"execution":{"iopub.status.busy":"2024-04-15T08:25:40.669787Z","iopub.execute_input":"2024-04-15T08:25:40.670864Z","iopub.status.idle":"2024-04-15T08:25:41.324785Z","shell.execute_reply.started":"2024-04-15T08:25:40.670828Z","shell.execute_reply":"2024-04-15T08:25:41.323489Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**SURF** the Speeded Up Robust Features (SURF) algorithm is a feature detection and description method for images. It is as robust and fast algorithm that is often used in computer vision applications, such as object recognition and image registration. SURF is considered to be a \"speeded up\" version of the Scale-Invariant Feature Transform (SIFT) algorithm. Its computational efficiency makes it more useful than SIFT for real-time applications. In SIFT, Lowe approximated Laplacian of Gaussian with Difference of Gaussian for finding scale-space. SURF goes a little further and apprroximates LoF with Box Filter. Below image shows a demonstration of such an approximation. One big advantage of this approxiamtion is that, convolution with box filter can be easily calculated with the help of integral images. And it can be done in parallel for different sclales. Also the SURF relu on determinant of Hessian matrix for both sclale and location.","metadata":{}},{"cell_type":"markdown","source":"**Viola-Jones** is a computer vision algorithm for object detection, specifically for detecting faces in images. It was developed by Paul Viola and Michael Jones in 2001. The algorithm uses a technique called \"integral image\" that allows for fast computation of Haar features, which are used to match features of typical human faces. The algorithm also uses a \"cascading classifiers\", which is a group of Haar-like features, to make predictions about whether a face is present in an image. The Viola-Jones algorithm is particularly efficient and is widely used in many applications such as security systems, photo tagging where computational power is limited.","metadata":{}},{"cell_type":"markdown","source":"**Eigenfaces** - is a computer vision algorithm that was developedin the early 1990s by researchers at MIT to recognize faces in images. The algorithm is based on the concept of eigenvectors. The algorithm first performs Principle Component Analysis (PCA) on a large set of face images, which are then used as set of \"eigenfaces\". The basic idea is that any face can be represented as a linear combination of these eigenfaces, and the coefficients of the linear combination can be used as a uniquie feature vector for the face.","metadata":{}},{"cell_type":"markdown","source":"**Histogram of Oriented Gradients (HOG)** - is a feature descriptor used in computer vision for object detection. It is used to represent the shape of an object by encoding the distriburion of intensity gradients or edge directions within an image. The basic idea behind HOG is to divide an image into small connected regions called cells, typically $ 8 \\times 8$ pixels, and then compute a histogram of gradient orientations for each cell. The histograms for all the cells in the image are then concatenated to create a feature vector for the entire image. This feature vector captures information about the object's shape and texture, which can be used as input to a machine learning algorithm for object detection.","metadata":{}},{"cell_type":"code","source":"from skimage.feature import hog\nfrom skimage import data, exposure\n\nimage = cv2.imread ('/kaggle/input/lego-minifigure-faces/00/0033.jpg')\n\nfd, hog_image = hog (image, orientations = 8,\n                    pixels_per_cell = (16, 16),\n                    cells_per_block = (1, 1),\n                    visualize = True,\n                    channel_axis = - 1,)\n\nfig, (ax1, ax2) = plt.subplots (1, 2, figsize = (8, 4), sharex = True, sharey = True)\n\nax1.axis ('off')\nax1.imshow (image, cmap = plt.cm.gray)\nax1.set_title ('Input image')\n\n# rescale histogram for better display\nhog_image_rescaled = exposure.rescale_intensity (hog_image, in_range = (0, 10))\n\nax2.axis ('off')\nax2.imshow (hog_image_rescaled, cmap = plt.cm.gray)\nax2.set_title ('Histogram of oriented gradients')\nplt.show ()","metadata":{"execution":{"iopub.status.busy":"2024-04-13T14:39:33.979779Z","iopub.execute_input":"2024-04-13T14:39:33.980181Z","iopub.status.idle":"2024-04-13T14:39:36.585737Z","shell.execute_reply.started":"2024-04-13T14:39:33.980152Z","shell.execute_reply":"2024-04-13T14:39:36.584922Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**YOLO (You Only Look Once)** - is a computer vision algorithm used for object detection in images and videos. It can process images and make predictions about the objects within them in a single pass, rather than requiring multiple passes thorugh the image, as is the case with other object detection algorithms. YOLO uses a convolutional neural network (CNN) to analyze the image and make predictions about the objects within it. It divides the image into a greid of cells. If the center of an object falls into a grid cell, then that grid cell is responsible for detecting that object. Each grid cell predicts a fixed number of bounding boxes, and produces confidence scores for those boxes. This allows YOLO to make predictions about multiple objects within the same image. ","metadata":{}},{"cell_type":"code","source":"!git clone https://github.com/ultralytics/yolov5\n%cd yolov5\n%pip install -qr requirements.txt\n\nimport torch\nimport utils\ndisplay = utils.notebook_init ()","metadata":{"execution":{"iopub.status.busy":"2024-04-13T14:53:16.79259Z","iopub.execute_input":"2024-04-13T14:53:16.792986Z","iopub.status.idle":"2024-04-13T14:53:48.828245Z","shell.execute_reply.started":"2024-04-13T14:53:16.792952Z","shell.execute_reply":"2024-04-13T14:53:48.827208Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!python classify/predict.py --weights yolov5s-cls.pt --img 224 --source data/images","metadata":{"execution":{"iopub.status.busy":"2024-04-13T15:23:22.550773Z","iopub.execute_input":"2024-04-13T15:23:22.551651Z","iopub.status.idle":"2024-04-13T15:23:32.371241Z","shell.execute_reply.started":"2024-04-13T15:23:22.551609Z","shell.execute_reply":"2024-04-13T15:23:32.369894Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**ResNet** - short for Residual Network is a deep convolutional neural network architecture that was developed by researchers at Microsoft in 2015. It's known for its performance on image classification and object detection tasks. The key innovation in ResNet is the use of \"residual connections\" between layers. This enables the network to better handle the vanishing gradient problem, which is common issue in very deep neural networks.","metadata":{}},{"cell_type":"markdown","source":"**Graph Cut Optimization** - grap cut algorithms are most commonly used in image segmentation to separate an image into multiple regions or segments based on color or texture. First, a network flow grapg is built based on the input image. The graph cut algorithm is a method for partitioning a graph into two or more sets of vertices (also called nodes). The goal is to minimize the number of edges that need to be cut, while ensuring that the verticesd in each subset satisfy certain conditions.","metadata":{}},{"cell_type":"markdown","source":"**Adaptive Image Thresholding** - adaptive thresholding can segment an image by setting all pixels whose intensity values are above a threshold to a foreground value and all the remaining pixels to a background value. The basic idea of adaptive thresholding is to use different threshold values for different regions of the image, rather than using a global threshold value for the entire image. This allows for the algorithm to take into account variations in the image's lightning and texture to produce a more accurate binary representation of the image.","metadata":{}},{"cell_type":"code","source":"# realised in notebook - Computer vision with OpenCV","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Lucas-Kanade** - this algorithm is a widely used method for optical flow estimation, which is the process of finding pixel-wise motions between consecutive images. The algorithm is based on the assumption that the optical flow in the local neighborhood of the pixels in an image is constant. It uses the brightness constancy assumption, meaning that the pixels in the image can move around, but their brightness cannot change. The Lucas-Kanade algorithm makes an estimate of the displacement of a neighborhood by looking at changes in pixel intensity. Such changes can be explained by the known intensity gradients of the image in that neoghborhood. Lucas-Kanade uses least-square estimation to find the optical flow of all the pixels over the neighborhood","metadata":{}},{"cell_type":"markdown","source":"**Kalman Filter** - is a mathematical algorithm that is commonly used to estimate the state of a system based on a series of noisy measurements. It is a recursive algorithm that uses a combination of predictions and measurements to estimate the state of the system at any point in time. These predictions are then compared to new measurements, and the algorithm uses a process called \"update\" to refine its estimates. Kalman Filters are commonly used in computer vision applications, in particular for object tracking tasks. Object tracking algorithms draw a bounding box across specific objects in an image, and attempt to accurately redraw this bounding box in subsequent frames, as the object moves. Kalman Filters can be used to predict the current and future positions of an object, even when it is hidden by obstacles (known as occlusion)","metadata":{}},{"cell_type":"markdown","source":"**Mean Shift Algorithm** - is a non-parametric, density-based clustering method for finding the regions with high density modes (i.e., high density) in a dataset. Each pixel is first assigned an initial mean, which is itsealf. The algorithm iteratively places a window around the initial mean, and calculates the new mean of all the points within that window. The process repeats until the position of the mean no longer changes significantly. The mean shift algorithm can also be extended to classify the data poitns into different clusters based in their final positions.","metadata":{}},{"cell_type":"markdown","source":"**Autoencoders** - autoencoders are a type of artificial neural network used for unsupervised learning. They consist of an encoder and a decoder, where the encoder maps the input data to a lower-dimensional representation (also known as the latent space or bottleneck), and the decoder maps the lower-dimensional representation back to an output. The main goal of autoencoders is to learn a compact representation of the data, which can be used for various tasks, such as dimensionality reduction, anomaly detection and generating new data samples. Autoencoders can be trained using various loss functions. An example is reconstruction loss, which measures the difference between the input and reconstructed output.","metadata":{}},{"cell_type":"markdown","source":"**Brute-Force Matching with ORB descriptors**","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport cv2\nimport matplotlib.pyplot as plt\n\nimg1 = cv2.imread ('/kaggle/input/google-image-recognition-tutorial/building_1.jpg',\n                  cv2.IMREAD_GRAYSCALE) # query image\nimg2 = cv2.imread ('/kaggle/input/google-image-recognition-tutorial/building_2.jpg',\n                  cv2.IMREAD_GRAYSCALE) # train image\n\n#  initiate ORB detector\norb = cv2.ORB_create ()\n\n# find keypoints and descriptors with ORB\nkp1, des1 = orb.detectAndCompute (img1, None)\nkp2, des2 = orb.detectAndCompute (img2, None)\n\n# create BFMatcher object\nbf = cv2.BFMatcher (cv2.NORM_HAMMING, crossCheck = True)\n\n# match descriptors\nmatches = bf.match (des1, des2)\n\n# sort them in the order of their distance\nmatches = sorted (matches, key = lambda x:x.distance)\n\n# draw first 10 matches\nimg3 = cv2.drawMatches (img1, kp1, img2, kp2, matches [:10], None, flags = cv2.DrawMatchesFlags_NOT_DRAW_SINGLE_POINTS)\n\nplt.imshow (img3), plt.show ()","metadata":{"execution":{"iopub.status.busy":"2024-04-29T18:01:37.417397Z","iopub.execute_input":"2024-04-29T18:01:37.41787Z","iopub.status.idle":"2024-04-29T18:01:38.374265Z","shell.execute_reply.started":"2024-04-29T18:01:37.417834Z","shell.execute_reply":"2024-04-29T18:01:38.373458Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Brute-Force Matching with SIFT descriptors and Ratio test**","metadata":{}},{"cell_type":"code","source":"img1 = cv2.imread ('/kaggle/input/google-image-recognition-tutorial/roma_1.jpg',\n                  cv2.IMREAD_GRAYSCALE) # query image\nimg2 = cv2.imread ('/kaggle/input/google-image-recognition-tutorial/roma_2.jpg',\n                  cv2.IMREAD_GRAYSCALE) # train image\n\n# initiate SIFT detector\nsift = cv2.SIFT_create ()\n\n# find the keypoints and descriptors with SIFT\nkp1, des1 = sift.detectAndCompute (img1, None)\nkp2, des2 = sift.detectAndCompute (img2, None)\n\n# BFMatcher with default params\nbf = cv2.BFMatcher ()\nmatches = bf.knnMatch (des1, des2, k = 2)\n\n# apply ratio test\ngood = []\nfor m, n in matches:\n    if m.distance < 0.75 * n.distance:\n        good.append ([m])\n        \n# cv2.drawMatchesKnn expects list of lists as matches\nimg3 = cv2.drawMatchesKnn (img1, kp1, img2, kp2, good, None, flags = cv2.DrawMatchesFlags_NOT_DRAW_SINGLE_POINTS)\n\nplt.imshow (img3), plt.show ()","metadata":{"execution":{"iopub.status.busy":"2024-04-29T18:01:45.69193Z","iopub.execute_input":"2024-04-29T18:01:45.693203Z","iopub.status.idle":"2024-04-29T18:01:54.242199Z","shell.execute_reply.started":"2024-04-29T18:01:45.693153Z","shell.execute_reply":"2024-04-29T18:01:54.240989Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**FLANN based Matcher**","metadata":{}},{"cell_type":"code","source":"img1 = cv2.imread ('/kaggle/input/google-image-recognition-tutorial/roma_1.jpg',\n                  cv2.IMREAD_GRAYSCALE) # query image\nimg2 = cv2.imread ('/kaggle/input/google-image-recognition-tutorial/roma_2.jpg',\n                  cv2.IMREAD_GRAYSCALE) # train image\n\n# initiate SIFT detector\nsift = cv2.SIFT_create ()\n\n# find the keypoints and descriptors with SIFT\nkp1, des1 = sift.detectAndCompute (img1, None)\nkp2, des2 = sift.detectAndCompute (img2, None)\n\n# FLANN parameters\nFLANN_INDEX_KDTREE = 1\nindex_params = dict (algorithm = FLANN_INDEX_KDTREE, trees = 5)\nsearch_params = dict (checks = 50) # or pass empty dictionary\n\nflann = cv2.FlannBasedMatcher (index_params, search_params)\n\nmatches = flann.knnMatch (des1, des2, k = 2)\n\n# need to draw only good matches, so create a mask\nmatchesMask = [[0, 0] for i in range (len (matches))]\n\n# ratio test as per Lowe's paper\nfor i, (m, n) in enumerate (matches):\n    if m.distance < 0.7 * n.distance:\n        matchesMask [i] = [1, 0]\n        \ndraw_params = dict (matchColor = (0, 255, 0),\n                   singlePointColor = (255, 0, 0),\n                   matchesMask = matchesMask,\n                   flags = cv2.DrawMatchesFlags_DEFAULT)\n\nimg3 = cv2.drawMatchesKnn (img1, kp1, img2, kp2, matches, None, **draw_params)\n\nplt.imshow (img3,), plt.show ()","metadata":{"execution":{"iopub.status.busy":"2024-04-29T18:17:52.588959Z","iopub.execute_input":"2024-04-29T18:17:52.590468Z","iopub.status.idle":"2024-04-29T18:17:56.717059Z","shell.execute_reply.started":"2024-04-29T18:17:52.590424Z","shell.execute_reply":"2024-04-29T18:17:56.716136Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Preprocessing of images**","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nprint (tf.version.VERSION)\ndevice_name = tf.test.gpu_device_name ()\nif device_name != '/device:GPU:0':\n    raise SystemError ('GPU device not found')\nprint ('Found GPU at: {}'.format (device_name))","metadata":{"execution":{"iopub.status.busy":"2024-05-12T16:07:54.349076Z","iopub.execute_input":"2024-05-12T16:07:54.349412Z","iopub.status.idle":"2024-05-12T16:08:09.64758Z","shell.execute_reply.started":"2024-05-12T16:07:54.349385Z","shell.execute_reply":"2024-05-12T16:08:09.646694Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport numpy as np\nimport tensorflow as tf\n\n# helper function\ndef training_plot (metrics, history):\n    f, ax = plt.subplots (1, len (metrics), figsize = (5 * len (metrics), 5))\n    for idx, metric in enumerate (mertrics):\n        ax[idx].plot (history.history [metric], ls = 'dashed')\n        ax[idx].set_xlabel ('Epochs')\n        ax[idx].set_ylabel (metric)\n        ax[idx].plot (history.history ['val_' + metric])\n        ax[idx].legend ([metric, 'val_' + metric])\n    \nIMG_HEIGHT = 224\nIMG_WIDTH = 224\nIMG_CHANNELS = 3\nCLASS_NAMES = 'astronomy picture'.split ()","metadata":{"execution":{"iopub.status.busy":"2024-05-12T16:09:15.018272Z","iopub.execute_input":"2024-05-12T16:09:15.018935Z","iopub.status.idle":"2024-05-12T16:09:15.027322Z","shell.execute_reply.started":"2024-05-12T16:09:15.018901Z","shell.execute_reply":"2024-05-12T16:09:15.026239Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# reading the images\ndef parse_tfr (proto):\n    feature_description = {\n        'image': tf.io.VarLenFeature (tf.float32),\n        'shape': tf.io.VarLenFeature (tf.int64),\n        'label': tf.io.FixedLenFeature ([], tf.string, default_value = ''),\n        'label_int': tf.io.FixedLenFeature ([], tf.int64, default_value = 0)\n    }\n    rec = tf.io.parse_single_example (\n    proto, feature_description)\n    shape = tf.sparse.to_dense (rec ['shape'])\n    img = tf.reshape (tf.sparse.to_dense (rec['image']), shape)\n    label_int = rec ['label_int']\n    return img, label_int\n\ntrain_dataset = tf.data.TFRecordDataset (\n[filename for filename in tf.io.gfile.glob (\n'gs://practical-ml-vision-book/flowers_tfr/train-*')], compression_type='GZIP').map (parse_tfr)\n\nf, ax = plt.subplots (1, 5, figsize = (15, 15))\nfor idx, (img, label_int) in enumerate (train_dataset.take (5)):\n    print (img.shape)\n    mean_of_image = tf.reduce_mean (img)\n    print (mean_of_image)\n    print (label_int)\n    ax[idx].imshow ((img.numpy ()))\n    ax[idx].set_title ('{}x{}'.format (img.shape [0], img.shape [1]))","metadata":{"execution":{"iopub.status.busy":"2024-05-12T16:09:17.802252Z","iopub.execute_input":"2024-05-12T16:09:17.802629Z","iopub.status.idle":"2024-05-12T16:09:20.180496Z","shell.execute_reply.started":"2024-05-12T16:09:17.802599Z","shell.execute_reply":"2024-05-12T16:09:20.179555Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"! pip install --upgrade tf-models-official","metadata":{"execution":{"iopub.status.busy":"2024-05-12T16:15:11.166026Z","iopub.execute_input":"2024-05-12T16:15:11.166715Z","iopub.status.idle":"2024-05-12T16:15:36.815626Z","shell.execute_reply.started":"2024-05-12T16:15:11.166682Z","shell.execute_reply":"2024-05-12T16:15:36.814047Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"preproc_layers = tf.keras.Sequential ([\n    tf.keras.layers.experimental.preprocessing.Resizing (\n    height = IMG_HEIGHT, width = IMG_WIDTH,\n    input_shape = (None, None, 3))\n])\n\ndef apply_preproc (img, label):\n    # add to a batch, call preproc, remove batch\n    x = tf.expand_dims (img, 0)\n    x = preproc_layers (x)\n    x = tf.squeeze (x, 0)\n    return x, label\n\nf, ax = plt.subplots (1, 5, figsize = (15, 15))\nfor idx, (img, label_int) in enumerate (train_dataset.map (apply_preproc).take (5)):\n    ax[idx].imshow ((img.numpy ()))","metadata":{"execution":{"iopub.status.busy":"2024-05-12T16:15:47.671918Z","iopub.execute_input":"2024-05-12T16:15:47.672816Z","iopub.status.idle":"2024-05-12T16:15:47.714201Z","shell.execute_reply.started":"2024-05-12T16:15:47.672776Z","shell.execute_reply":"2024-05-12T16:15:47.71314Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Transfer learning with MobileNet**","metadata":{}},{"cell_type":"code","source":"import tensorflow_hub as hub\nimport os\n# load compressed models from tensorflow_hub\nos.environ ['TFHUB_MODEL_LOAD_FORMAT'] = 'COMPRESSED'\n\npreproc_layes = tf.keras.Sequential ([\n    tf.keras.layers.Lambda (lambda img:\n                           tf.image.resize_with_pad (\n                           img, 2*IMG_HEIGHT, 2*IMG_WIDTH),\n                           input_shape = (None, None, 3)),\n    tf.keras.layers.experimental.preprocessing.CenterCrop (\n    height = IMG_HEIGHT, width = IMG_WIDTH)\n])\n\ndef apply_preproc (img, label):\n    # add to a batch, call preproc, remove from batch\n    x = tf.expand_dims (img, 0)\n    x = preproc_layers (x)\n    x = tf.squeeze (x, 0)\n    return x, label\n\n# parametrize to the values in the previous cell\ndef train_and_evaluate (batch_size = 32, lrate = 0.001, l1 = 0., l2 = 0.,\n                       num_hidden = 16):\n    regularizer = tf.keras.regularizers.l1_l2 (l1, l2)\n    \n    train_dataset = tf.data.TFRecordDataset (\n    [filename for filename in tf.io.gfile.glob (\n    'gs://practical-ml-vision-book/flowers_tfr/train-*')],\n    compression_type = 'GZIP').map (parse_tfr).map (apply_preproc).batch (batch_size)","metadata":{},"execution_count":null,"outputs":[]}]}