{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":6664020,"sourceType":"datasetVersion","datasetId":3845355}],"dockerImageVersionId":30698,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/aleksandrmorozov123/video-detection-algorithms?scriptVersionId=176698339\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-05-09T18:19:09.571432Z","iopub.execute_input":"2024-05-09T18:19:09.571832Z","iopub.status.idle":"2024-05-09T18:19:10.032936Z","shell.execute_reply.started":"2024-05-09T18:19:09.571798Z","shell.execute_reply":"2024-05-09T18:19:10.031805Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"/kaggle/input/aggressive-behavior-video-classification/aggressive_behavior.csv\n/kaggle/input/aggressive-behavior-video-classification/files/aggressive/3.mp4\n/kaggle/input/aggressive-behavior-video-classification/files/aggressive/1.mp4\n/kaggle/input/aggressive-behavior-video-classification/files/aggressive/4.mp4\n/kaggle/input/aggressive-behavior-video-classification/files/aggressive/0.mp4\n/kaggle/input/aggressive-behavior-video-classification/files/aggressive/2.mp4\n/kaggle/input/aggressive-behavior-video-classification/files/non_aggressive/5.mp4\n/kaggle/input/aggressive-behavior-video-classification/files/non_aggressive/3.mp4\n/kaggle/input/aggressive-behavior-video-classification/files/non_aggressive/1.mp4\n/kaggle/input/aggressive-behavior-video-classification/files/non_aggressive/4.mp4\n/kaggle/input/aggressive-behavior-video-classification/files/non_aggressive/0.mp4\n/kaggle/input/aggressive-behavior-video-classification/files/non_aggressive/2.mp4\n","output_type":"stream"}]},{"cell_type":"markdown","source":"**Preparing the data**","metadata":{}},{"cell_type":"code","source":"# installing required packages\n!pip install git+https://github.com/sajjjadayobi/FaceLib.git\n!git clone https://github.com/Pongpisit-Thanasutives/Variations-of-SFANet-for-Crowd-Counting","metadata":{"execution":{"iopub.status.busy":"2024-05-09T18:19:10.035122Z","iopub.execute_input":"2024-05-09T18:19:10.035682Z","iopub.status.idle":"2024-05-09T18:19:30.105375Z","shell.execute_reply.started":"2024-05-09T18:19:10.035643Z","shell.execute_reply":"2024-05-09T18:19:30.104218Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Collecting git+https://github.com/sajjjadayobi/FaceLib.git\n  Cloning https://github.com/sajjjadayobi/FaceLib.git to /tmp/pip-req-build-stki1ez9\n  Running command git clone --filter=blob:none --quiet https://github.com/sajjjadayobi/FaceLib.git /tmp/pip-req-build-stki1ez9\n  Resolved https://github.com/sajjjadayobi/FaceLib.git to commit 382841efec823f059f49754e5d378abb12cfb551\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: torch>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from facelib==1.6) (2.1.2+cpu)\nRequirement already satisfied: numpy>=1.14.5 in /opt/conda/lib/python3.10/site-packages (from facelib==1.6) (1.26.4)\nRequirement already satisfied: matplotlib>=2.1.2 in /opt/conda/lib/python3.10/site-packages (from facelib==1.6) (3.7.5)\nRequirement already satisfied: tqdm>=4.23.4 in /opt/conda/lib/python3.10/site-packages (from facelib==1.6) (4.66.1)\nRequirement already satisfied: easydict>=1.7 in /opt/conda/lib/python3.10/site-packages (from facelib==1.6) (1.13)\nRequirement already satisfied: opencv_python>=3.4.0.12 in /opt/conda/lib/python3.10/site-packages (from facelib==1.6) (4.9.0.80)\nRequirement already satisfied: torchvision>=0.2.1 in /opt/conda/lib/python3.10/site-packages (from facelib==1.6) (0.16.2+cpu)\nRequirement already satisfied: scikit-image in /opt/conda/lib/python3.10/site-packages (from facelib==1.6) (0.22.0)\nRequirement already satisfied: scikit-learn in /opt/conda/lib/python3.10/site-packages (from facelib==1.6) (1.2.2)\nRequirement already satisfied: scipy in /opt/conda/lib/python3.10/site-packages (from facelib==1.6) (1.11.4)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from facelib==1.6) (2.31.0)\nRequirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=2.1.2->facelib==1.6) (1.2.0)\nRequirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=2.1.2->facelib==1.6) (0.12.1)\nRequirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=2.1.2->facelib==1.6) (4.47.0)\nRequirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=2.1.2->facelib==1.6) (1.4.5)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=2.1.2->facelib==1.6) (21.3)\nRequirement already satisfied: pillow>=6.2.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=2.1.2->facelib==1.6) (9.5.0)\nRequirement already satisfied: pyparsing>=2.3.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=2.1.2->facelib==1.6) (3.1.1)\nRequirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=2.1.2->facelib==1.6) (2.9.0.post0)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=0.4.0->facelib==1.6) (3.13.1)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch>=0.4.0->facelib==1.6) (4.9.0)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=0.4.0->facelib==1.6) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=0.4.0->facelib==1.6) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=0.4.0->facelib==1.6) (3.1.2)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch>=0.4.0->facelib==1.6) (2024.2.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->facelib==1.6) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->facelib==1.6) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->facelib==1.6) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->facelib==1.6) (2024.2.2)\nRequirement already satisfied: imageio>=2.27 in /opt/conda/lib/python3.10/site-packages (from scikit-image->facelib==1.6) (2.33.1)\nRequirement already satisfied: tifffile>=2022.8.12 in /opt/conda/lib/python3.10/site-packages (from scikit-image->facelib==1.6) (2023.12.9)\nRequirement already satisfied: lazy_loader>=0.3 in /opt/conda/lib/python3.10/site-packages (from scikit-image->facelib==1.6) (0.3)\nRequirement already satisfied: joblib>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->facelib==1.6) (1.4.0)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->facelib==1.6) (3.2.0)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib>=2.1.2->facelib==1.6) (1.16.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=0.4.0->facelib==1.6) (2.1.3)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=0.4.0->facelib==1.6) (1.3.0)\nBuilding wheels for collected packages: facelib\n  Building wheel for facelib (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for facelib: filename=facelib-1.6-py3-none-any.whl size=44144 sha256=f541f816a9fbb3068e24ad75ea77789f6b29694137d758ace4442b23885d8f35\n  Stored in directory: /tmp/pip-ephem-wheel-cache-_se7kfwf/wheels/54/7b/e1/15419c5fb2b8e868bddef5dbcbaf30c4901ee9078e228cd4b8\nSuccessfully built facelib\nInstalling collected packages: facelib\nSuccessfully installed facelib-1.6\nCloning into 'Variations-of-SFANet-for-Crowd-Counting'...\nremote: Enumerating objects: 161, done.\u001b[K\nremote: Counting objects: 100% (31/31), done.\u001b[K\nremote: Compressing objects: 100% (18/18), done.\u001b[K\nremote: Total 161 (delta 8), reused 29 (delta 6), pack-reused 130\u001b[K\nReceiving objects: 100% (161/161), 14.04 MiB | 22.85 MiB/s, done.\nResolving deltas: 100% (56/56), done.\n","output_type":"stream"}]},{"cell_type":"code","source":"# import required libraries\nimport cv2\nfrom PIL import Image\nimport pandas as pd\nimport numpy as np\n%matplotlib inline\nimport matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\nimport glob\nimport torch\nfrom torchvision import transforms\n\n#import model\nimport os\nos.chdir('/kaggle/working/Variations-of-SFANet-for-Crowd-Counting')\nfrom models import M_SFANet_UCF_QNRF\n\n# import facelib\nfrom facelib import FaceDetector, AgeGenderEstimator","metadata":{"execution":{"iopub.status.busy":"2024-05-09T18:19:30.107104Z","iopub.execute_input":"2024-05-09T18:19:30.107471Z","iopub.status.idle":"2024-05-09T18:19:37.618515Z","shell.execute_reply.started":"2024-05-09T18:19:30.107438Z","shell.execute_reply":"2024-05-09T18:19:37.617142Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"# convert the video to a series of images\ndef video_to_image (path, folder):\n    global exp_fld\n    \n    # importing video\n    vidcap = cv2.VideoCapture (path)\n    exp_fld = folder\n    \n    # error handling\n    try:\n        if not os.path.exists (exp_fld):\n            os.makedirs (exp_fld)\n    except OSError:\n        print ('Error: Creating directory of data')\n    Count = 0\n    sec = 0\n    frameRate = 1 # secs of the video\n    \n    while (True):\n        vidcap.set (cv2.CAP_PROP_POS_MSEC, sec * 1000)\n        hasFrames, image = vidcap.read ()\n        sec = sec + frameRate\n        sec = round (sec, 2)\n        \n        # exporting the image\n        if hasFrames:\n            name = './' + exp_fld + '/frame' + str (Count) + '.jpg'\n            cv2.imwrite (name, image)\n            Count += 1\n        else:\n            break\n    return print ('Image Exported')\n\n# setting the path\npath = ('/kaggle/input/aggressive-behavior-video-classification/files/aggressive')\n\n# extracting images and storing for a first video\nvideo_to_image ('/kaggle/input/aggressive-behavior-video-classification/files/aggressive/0.mp4', 'crowd')\n\n# extracting images for forth video\nvideo_to_image ('/kaggle/input/aggressive-behavior-video-classification/files/aggressive/3.mp4', 'movement')\n","metadata":{"execution":{"iopub.status.busy":"2024-05-09T18:19:37.619897Z","iopub.execute_input":"2024-05-09T18:19:37.620411Z","iopub.status.idle":"2024-05-09T18:39:05.926839Z","shell.execute_reply.started":"2024-05-09T18:19:37.620379Z","shell.execute_reply":"2024-05-09T18:39:05.924538Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"Image Exported\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[4], line 41\u001b[0m\n\u001b[1;32m     38\u001b[0m video_to_image (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/kaggle/input/aggressive-behavior-video-classification/files/aggressive/0.mp4\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcrowd\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     40\u001b[0m \u001b[38;5;66;03m# extracting images for forth video\u001b[39;00m\n\u001b[0;32m---> 41\u001b[0m \u001b[43mvideo_to_image\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/kaggle/input/aggressive-behavior-video-classification/files/aggressive/3.mp4\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmovement\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n","Cell \u001b[0;32mIn[4], line 20\u001b[0m, in \u001b[0;36mvideo_to_image\u001b[0;34m(path, folder)\u001b[0m\n\u001b[1;32m     17\u001b[0m frameRate \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;66;03m# secs of the video\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m (\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m---> 20\u001b[0m     \u001b[43mvidcap\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mset\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCAP_PROP_POS_MSEC\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msec\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m     hasFrames, image \u001b[38;5;241m=\u001b[39m vidcap\u001b[38;5;241m.\u001b[39mread ()\n\u001b[1;32m     22\u001b[0m     sec \u001b[38;5;241m=\u001b[39m sec \u001b[38;5;241m+\u001b[39m frameRate\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}]},{"cell_type":"code","source":"# resizing the image\ndef img_re_sizing (dnst_mp, image):\n    # normalizing\n    dnst_mp = 255 * dnst_mp / np.max (dnst_mp)\n    dnst_mp = dnst_mp [0][0]\n    image = image [0]\n    \n    # empty image\n    result_img = np.zeros ((dnst_mp.shape [0] * 2, dnst_mp.shape [1] * 2))\n    \n    # iterate for each image\n    for i in range (result_img.shape [0]):\n        for j in range (result_img.shape [1]):\n            result_img [i][j] = dnst_mp [int (i / 2)]\n            [int (j / 2)] /3\n    result_img = result_img.astype (np.uint8, copy = False)\n    \n    # output\n    return result_img","metadata":{"execution":{"iopub.status.busy":"2024-05-09T18:39:05.928157Z","iopub.status.idle":"2024-05-09T18:39:05.928703Z","shell.execute_reply.started":"2024-05-09T18:39:05.928504Z","shell.execute_reply":"2024-05-09T18:39:05.928524Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# function to get a heatmap\ndef generate_dstys_map (o, dsty, cc, image_location):\n    # define the fgr_imure\n    fgr_im = plt.fgr_imure ()\n    \n    # define size\n    col = 2\n    rws = 1\n    X = o\n    \n    # sum \n    add = int (np.sum (dsty))\n    \n    dsty = image_re_sizing (dsty, o)\n    \n    # adding original image and new generated heatmap image\n    for i in range (1, col * rws + 1):\n        # generate original image\n        if i == 1:\n            image = X\n            fgr_im.add_subplot (rws, col, i)\n            # setting axis\n            plt.gca().set_axis_off ()\n            plt.margins (0, 0)\n            \n            #locator\n            plt.gca().xaxis.set_major_locator (plt.NullLocator ())\n            plt.gca().yaxis.set_major_locator (plt.NullLocator ())\n            # adjusting subplots\n            plt.subplots_adjust (top = 1, bottom = 0, right = 1, left = 0,\n                                hspace = 0, wspace = 0)\n            # show image\n            plt.imshow (image)\n    # generate dstys image\n    if i == 2:\n        image = dsty\n        fgr_im.add_subplot (rws, col, i)\n        # setting axis\n        plt.gca().set_axis_off ()\n        plt.margins (0, 0)\n        # locator\n        plt.gca().xaxis.set_major_locator (plt.NullLocator ())\n        plt.gca().yaxis.set_major_locator (plt.NullLocator ())\n        # adjusting subplots\n        plt.subplots_adjust (top = 1, bottom = 0, right = 1, left = 0,\n                            hspace = 0, wspace = 0)\n        # adding count\n        plt.text (1, 80, 'M-SegNet * Est: '+ str (add)+',\n                 Gt: '+str (cc), fontsize = 7, weight = \"bold', color = 'w')\n        # show image \n        plt.imshow (image), cmap = CM.jet\n        \n# image_nm with location\nimage_nm = image_location.split ('/')[-1]\nimage_nm = image_nm.replace ('.jpg', '_heatmap.png')\n\n# saving the image\nplt.savefgr_im (image_location.split (image_nm)\n                [0]+'seg_'+ image_nm, transparent = True\n                bbox_inches = 'tight', pad_inches = 0.0, dpi = 200)","metadata":{"execution":{"iopub.status.busy":"2024-05-09T18:39:05.929652Z","iopub.status.idle":"2024-05-09T18:39:05.930025Z","shell.execute_reply.started":"2024-05-09T18:39:05.929851Z","shell.execute_reply":"2024-05-09T18:39:05.929867Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#  getting count of peoples\ndef get_count_people (image):\n    # simple preprocessing\n    trans  = transforms.Compose ([transforms,ToTensor (),\n                                 transforms.Normalize ([0.485, 0.456, 0.406],\n                                                      [0.229, 0.224, 0.225])])\n    # sample image with height and width\n    img = Image.open (image).convert ('RGB')\n    height, width = img.size [1], img.size [0]\n    height = round (height / 16) * 16\n    width = round (width / 16) * 16\n    \n    # resize the image\n    img_den = cv2.resize (np.array (img), (width, height), cv2.INTER_CUBIC)\n    \n    # transform \n    img = trans (Image.fromarray (img_den))[None, :]\n    \n    # lets define the model\n    model = M_SFANet_UCF_QNRF.Model ()\n    \n    # load the model\n    model.load_state_dict (torch.load ('/content/best_M-SFANet_UCF_QNRF.pth',\n                                      map_location = torch.device ('cpu')))\n    \n    # evaluating the model\n    model.eval ()\n    dnst_mp = model (img)\n    \n    # final count \n    count = torch.sum (dnst_mp).item ()\n    \n    # return count, density and map\n    return count, img_den, dnst_mp","metadata":{"execution":{"iopub.status.busy":"2024-05-09T18:39:05.931272Z","iopub.status.idle":"2024-05-09T18:39:05.931693Z","shell.execute_reply.started":"2024-05-09T18:39:05.931504Z","shell.execute_reply":"2024-05-09T18:39:05.931521Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# importing the images\nimage_location = []\npath_sets = ['/content/crowd']\n\n# loading all the images\nfor path in path_sets:\n    for img_path in glob.glob (os.path.join (path, '*.jpg')):\n        image_location.append (img_path)\nimage_location [:3]","metadata":{"execution":{"iopub.status.busy":"2024-05-09T18:39:05.932735Z","iopub.status.idle":"2024-05-09T18:39:05.933119Z","shell.execute_reply.started":"2024-05-09T18:39:05.932944Z","shell.execute_reply":"2024-05-09T18:39:05.932959Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# getting the count of people by image\n# define empty list\nlist_df = []\n\n# loop through each image\nfor i in image_location:\n    count, img_den, dnst_mp = get_count_people (i)\n    generate_dens_map (img_den, dnst_mp.cpu().detach ().numpy(), 0, i)\n    list_df = list_df + [[i, count]]\n    \n# create the dataframe with image id and count\ndf = pd.DataFrame (list_df, columns = ['image', 'count'])\n\n# sort and show\ndf.sort_values (['image']).head ()\n\ndf.describe()","metadata":{"execution":{"iopub.status.busy":"2024-05-09T18:39:05.934113Z","iopub.status.idle":"2024-05-09T18:39:05.934469Z","shell.execute_reply.started":"2024-05-09T18:39:05.934297Z","shell.execute_reply":"2024-05-09T18:39:05.934312Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import seaborn as sns\nsns.histplot (data = df['count'])","metadata":{"execution":{"iopub.status.busy":"2024-05-09T18:39:05.935497Z","iopub.status.idle":"2024-05-09T18:39:05.935908Z","shell.execute_reply.started":"2024-05-09T18:39:05.935672Z","shell.execute_reply":"2024-05-09T18:39:05.935686Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# getting all the images from the path\nimage_location = []\npath_sets = ['content/movement']\n\n# loading all images\nfor path in path_sets:\n    for img_path in glob.glob (os.path.join (path, '*.jpg')):\n        image_location.append (img_path)\n\nimage_location [:3]\n\n# getting the count of people by image\nlist_df = []","metadata":{"execution":{"iopub.status.busy":"2024-05-09T18:39:05.937326Z","iopub.status.idle":"2024-05-09T18:39:05.937781Z","shell.execute_reply.started":"2024-05-09T18:39:05.93759Z","shell.execute_reply":"2024-05-09T18:39:05.937605Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# checking if there is any movement for each frame in video\nfor i in image_location:\n    count, img_den, dnst_mp = get_count_people (i)\n    generate_dens_map (img_den, dnst_mp.cpu().detach().numpy(), o, i)\n    list_df = list_df + [[i, count]]\n    \n# detecting the data into dataframe\ndetected_df = pd.DataFrame (list_df, columns = ['image', 'count'])\ndetected_df ['movement'] = np.where (detected_df ['count'] > 3, 'yes', 'no')\ndetected_df.filter (items = [45, 56, 53], axis = 0)","metadata":{"execution":{"iopub.status.busy":"2024-05-09T18:39:05.938632Z","iopub.status.idle":"2024-05-09T18:39:05.939001Z","shell.execute_reply.started":"2024-05-09T18:39:05.938827Z","shell.execute_reply":"2024-05-09T18:39:05.938842Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# print the image where movement = 'yes'\nimage = '/content/movement/frame25.jpg'\nImage.open (image)\n\nimage = '/content/movement.frame26.jpg'\nImage.open (image)","metadata":{"execution":{"iopub.status.busy":"2024-05-09T18:39:05.941085Z","iopub.status.idle":"2024-05-09T18:39:05.941598Z","shell.execute_reply.started":"2024-05-09T18:39:05.941343Z","shell.execute_reply":"2024-05-09T18:39:05.941366Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# identify age and gender\nface_detector = FaceDetector ()\nage_gender_detector = AgeGenderEstimator ()\n\n# reading image\nimg = '/content/movement.frame1.jpg'\nimage = cv2.imread (img, cv2.IMREAD_UNCHANGED)\nImage.open (img)","metadata":{"execution":{"iopub.status.busy":"2024-05-09T18:39:05.943587Z","iopub.status.idle":"2024-05-09T18:39:05.944181Z","shell.execute_reply.started":"2024-05-09T18:39:05.943905Z","shell.execute_reply":"2024-05-09T18:39:05.943927Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# checking the gender \nfaces, boxes, scores, landmarks = face_detector.detect_align (image)\ngenders, ages = age_gender_detector.detect (faces)\nprint (genders, ages)","metadata":{"execution":{"iopub.status.busy":"2024-05-09T18:39:05.945505Z","iopub.status.idle":"2024-05-09T18:39:05.946399Z","shell.execute_reply.started":"2024-05-09T18:39:05.94612Z","shell.execute_reply":"2024-05-09T18:39:05.946144Z"},"trusted":true},"execution_count":null,"outputs":[]}]}