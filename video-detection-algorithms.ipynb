{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":6664020,"sourceType":"datasetVersion","datasetId":3845355}],"dockerImageVersionId":30698,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/aleksandrmorozov123/video-detection-algorithms?scriptVersionId=176056500\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-05-05T15:57:59.29785Z","iopub.execute_input":"2024-05-05T15:57:59.298382Z","iopub.status.idle":"2024-05-05T15:57:59.909462Z","shell.execute_reply.started":"2024-05-05T15:57:59.298339Z","shell.execute_reply":"2024-05-05T15:57:59.908078Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"/kaggle/input/aggressive-behavior-video-classification/aggressive_behavior.csv\n/kaggle/input/aggressive-behavior-video-classification/files/aggressive/3.mp4\n/kaggle/input/aggressive-behavior-video-classification/files/aggressive/1.mp4\n/kaggle/input/aggressive-behavior-video-classification/files/aggressive/4.mp4\n/kaggle/input/aggressive-behavior-video-classification/files/aggressive/0.mp4\n/kaggle/input/aggressive-behavior-video-classification/files/aggressive/2.mp4\n/kaggle/input/aggressive-behavior-video-classification/files/non_aggressive/5.mp4\n/kaggle/input/aggressive-behavior-video-classification/files/non_aggressive/3.mp4\n/kaggle/input/aggressive-behavior-video-classification/files/non_aggressive/1.mp4\n/kaggle/input/aggressive-behavior-video-classification/files/non_aggressive/4.mp4\n/kaggle/input/aggressive-behavior-video-classification/files/non_aggressive/0.mp4\n/kaggle/input/aggressive-behavior-video-classification/files/non_aggressive/2.mp4\n","output_type":"stream"}]},{"cell_type":"markdown","source":"**Preparing the data**","metadata":{}},{"cell_type":"code","source":"# installing required packages\n!pip install git+https://github.com/sajjjadayobi/FaceLib.git\n!git clone https://github.com/Pongpisit-Thanasutives/Variations-of-SFANet-for-Crowd-Counting","metadata":{"execution":{"iopub.status.busy":"2024-05-05T15:57:59.911984Z","iopub.execute_input":"2024-05-05T15:57:59.912978Z","iopub.status.idle":"2024-05-05T15:58:20.554251Z","shell.execute_reply.started":"2024-05-05T15:57:59.912884Z","shell.execute_reply":"2024-05-05T15:58:20.552696Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Collecting git+https://github.com/sajjjadayobi/FaceLib.git\n  Cloning https://github.com/sajjjadayobi/FaceLib.git to /tmp/pip-req-build-4ilopz4q\n  Running command git clone --filter=blob:none --quiet https://github.com/sajjjadayobi/FaceLib.git /tmp/pip-req-build-4ilopz4q\n  Resolved https://github.com/sajjjadayobi/FaceLib.git to commit 382841efec823f059f49754e5d378abb12cfb551\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: torch>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from facelib==1.6) (2.1.2+cpu)\nRequirement already satisfied: numpy>=1.14.5 in /opt/conda/lib/python3.10/site-packages (from facelib==1.6) (1.26.4)\nRequirement already satisfied: matplotlib>=2.1.2 in /opt/conda/lib/python3.10/site-packages (from facelib==1.6) (3.7.5)\nRequirement already satisfied: tqdm>=4.23.4 in /opt/conda/lib/python3.10/site-packages (from facelib==1.6) (4.66.1)\nRequirement already satisfied: easydict>=1.7 in /opt/conda/lib/python3.10/site-packages (from facelib==1.6) (1.13)\nRequirement already satisfied: opencv_python>=3.4.0.12 in /opt/conda/lib/python3.10/site-packages (from facelib==1.6) (4.9.0.80)\nRequirement already satisfied: torchvision>=0.2.1 in /opt/conda/lib/python3.10/site-packages (from facelib==1.6) (0.16.2+cpu)\nRequirement already satisfied: scikit-image in /opt/conda/lib/python3.10/site-packages (from facelib==1.6) (0.22.0)\nRequirement already satisfied: scikit-learn in /opt/conda/lib/python3.10/site-packages (from facelib==1.6) (1.2.2)\nRequirement already satisfied: scipy in /opt/conda/lib/python3.10/site-packages (from facelib==1.6) (1.11.4)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from facelib==1.6) (2.31.0)\nRequirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=2.1.2->facelib==1.6) (1.2.0)\nRequirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=2.1.2->facelib==1.6) (0.12.1)\nRequirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=2.1.2->facelib==1.6) (4.47.0)\nRequirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=2.1.2->facelib==1.6) (1.4.5)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=2.1.2->facelib==1.6) (21.3)\nRequirement already satisfied: pillow>=6.2.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=2.1.2->facelib==1.6) (9.5.0)\nRequirement already satisfied: pyparsing>=2.3.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=2.1.2->facelib==1.6) (3.1.1)\nRequirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=2.1.2->facelib==1.6) (2.9.0.post0)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=0.4.0->facelib==1.6) (3.13.1)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch>=0.4.0->facelib==1.6) (4.9.0)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=0.4.0->facelib==1.6) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=0.4.0->facelib==1.6) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=0.4.0->facelib==1.6) (3.1.2)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch>=0.4.0->facelib==1.6) (2024.2.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->facelib==1.6) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->facelib==1.6) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->facelib==1.6) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->facelib==1.6) (2024.2.2)\nRequirement already satisfied: imageio>=2.27 in /opt/conda/lib/python3.10/site-packages (from scikit-image->facelib==1.6) (2.33.1)\nRequirement already satisfied: tifffile>=2022.8.12 in /opt/conda/lib/python3.10/site-packages (from scikit-image->facelib==1.6) (2023.12.9)\nRequirement already satisfied: lazy_loader>=0.3 in /opt/conda/lib/python3.10/site-packages (from scikit-image->facelib==1.6) (0.3)\nRequirement already satisfied: joblib>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->facelib==1.6) (1.4.0)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->facelib==1.6) (3.2.0)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib>=2.1.2->facelib==1.6) (1.16.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=0.4.0->facelib==1.6) (2.1.3)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=0.4.0->facelib==1.6) (1.3.0)\nBuilding wheels for collected packages: facelib\n  Building wheel for facelib (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for facelib: filename=facelib-1.6-py3-none-any.whl size=44144 sha256=b3979887410b27cb463e4587dde16900f5416788c2675e43db1855efb772d31d\n  Stored in directory: /tmp/pip-ephem-wheel-cache-1bgvwpsw/wheels/54/7b/e1/15419c5fb2b8e868bddef5dbcbaf30c4901ee9078e228cd4b8\nSuccessfully built facelib\nInstalling collected packages: facelib\nSuccessfully installed facelib-1.6\nCloning into 'Variations-of-SFANet-for-Crowd-Counting'...\nremote: Enumerating objects: 161, done.\u001b[K\nremote: Counting objects: 100% (31/31), done.\u001b[K\nremote: Compressing objects: 100% (18/18), done.\u001b[K\nremote: Total 161 (delta 8), reused 29 (delta 6), pack-reused 130\u001b[K\nReceiving objects: 100% (161/161), 14.04 MiB | 45.06 MiB/s, done.\nResolving deltas: 100% (56/56), done.\n","output_type":"stream"}]},{"cell_type":"code","source":"# import required libraries\nimport cv2\nfrom PIL import Image\nimport pandas as pd\nimport numpy as np\n%matplotlib inline\nimport matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\nimport glob\nimport torch\nfrom torchvision import transforms\n\n#import model\nimport os\nos.chdir('/kaggle/working/Variations-of-SFANet-for-Crowd-Counting')\nfrom models import M_SFANet_UCF_QNRF\n\n# import facelib\nfrom facelib import FaceDetector, AgeGenderEstimator","metadata":{"execution":{"iopub.status.busy":"2024-05-05T15:58:20.556578Z","iopub.execute_input":"2024-05-05T15:58:20.557028Z","iopub.status.idle":"2024-05-05T15:58:28.684394Z","shell.execute_reply.started":"2024-05-05T15:58:20.556986Z","shell.execute_reply":"2024-05-05T15:58:28.683411Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"# convert the video to a series of images\ndef video_to_image (path, folder):\n    global exp_fld\n    \n    # importing video\n    vidcap = cv2.VideoCapture (path)\n    exp_fld = folder\n    \n    # error handling\n    try:\n        if not os.path.exists (exp_fld):\n            os.makedirs (exp_fld)\n    except OSError:\n        print ('Error: Creating directory of data')\n    Count = 0\n    sec = 0\n    frameRate = 1 # secs of the video\n    \n    while (True):\n        vidcap.set (cv2.CAP_PROP_POS_MSEC, sec * 1000)\n        hasFrames, image = vidcap.read ()\n        sec = sec + frameRate\n        sec = round (sec, 2)\n        \n        # exporting the image\n        if hasFrames:\n            name = './' + exp_fld + '/frame' + str (Count) + '.jpg'\n            cv2.imwrite (name, image)\n            Count += 1\n        else:\n            break\n    return print ('Image Exported')\n\n# setting the path\npath = ('/kaggle/input/aggressive-behavior-video-classification/files/aggressive')\n\n# extracting images and storing for a first video\nvideo_to_image ('/kaggle/input/aggressive-behavior-video-classification/files/aggressive/0.mp4', 'crowd')\n\n# extracting images for forth video\nvideo_to_image ('/kaggle/input/aggressive-behavior-video-classification/files/aggressive/3.mp4', 'movement')\n","metadata":{"execution":{"iopub.status.busy":"2024-05-05T16:37:29.269843Z","iopub.execute_input":"2024-05-05T16:37:29.271706Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"Error: Creating directory of data\nImage Exported\nError: Creating directory of data\n","output_type":"stream"}]},{"cell_type":"code","source":"# resizing the image\ndef img_re_sizing (dnst_mp, image):\n    # normalizing\n    dnst_mp = 255 * dnst_mp / np.max (dnst_mp)\n    dnst_mp = dnst_mp [0][0]\n    image = image [0]\n    \n    # empty image\n    result_img = np.zeros ((dnst_mp.shape [0] * 2, dnst_mp.shape [1] * 2))\n    \n    # iterate for each image\n    for i in range (result_img.shape [0]):\n        for j in range (result_img.shape [1]):\n            result_img [i][j] = dnst_mp [int (i / 2)]\n            [int (j / 2)] /3\n    result_img = result_img.astype (np.uint8, copy = False)\n    \n    # output\n    return result_img","metadata":{"execution":{"iopub.status.busy":"2024-05-05T16:36:26.200262Z","iopub.status.idle":"2024-05-05T16:36:26.200743Z","shell.execute_reply.started":"2024-05-05T16:36:26.200523Z","shell.execute_reply":"2024-05-05T16:36:26.200544Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# function to get a heatmap\ndef generate_dstys_map (o, dsty, cc, image_location):\n    # define the fgr_imure\n    fgr_im = plt.fgr_imure ()\n    \n    # define size\n    col = 2\n    rws = 1\n    X = o\n    \n    # sum \n    add = int (np.sum (dsty))\n    \n    dsty = image_re_sizing (dsty, o)\n    \n    # adding original image and new generated heatmap image\n    for i in range (1, col * rws + 1):\n        # generate original image\n        if i == 1:\n            image = X\n            fgr_im.add_subplot (rws, col, i)\n            # setting axis\n            plt.gca().set_axis_off ()\n            plt.margins (0, 0)\n            \n            #locator\n            plt.gca().xaxis.set_major_locator (plt.NullLocator ())\n            plt.gca().yaxis.set_major_locator (plt.NullLocator ())\n            # adjusting subplots\n            plt.subplots_adjust (top = 1, bottom = 0, right = 1, left = 0,\n                                hspace = 0, wspace = 0)\n            # show image\n            plt.imshow (image)\n    # generate dstys image\n    if i == 2:\n        image = dsty\n        fgr_im.add_subplot (rws, col, i)\n        # setting axis\n        plt.gca().set_axis_off ()\n        plt.margins (0, 0)\n        # locator\n        plt.gca().xaxis.set_major_locator (plt.NullLocator ())\n        plt.gca().yaxis.set_major_locator (plt.NullLocator ())\n        # adjusting subplots\n        plt.subplots_adjust (top = 1, bottom = 0, right = 1, left = 0,\n                            hspace = 0, wspace = 0)\n        # adding count\n        plt.text (1, 80, 'M-SegNet * Est: '+ str (add)+',\n                 Gt: '+str (cc), fontsize = 7, weight = \"bold', color = 'w')\n        # show image \n        plt.imshow (image), cmap = CM.jet\n        \n# image_nm with location\nimage_nm = image_location.split ('/')[-1]\nimage_nm = image_nm.replace ('.jpg', '_heatmap.png')\n\n# saving the image\nplt.savefgr_im (image_location.split (image_nm)\n                [0]+'seg_'+ image_nm, transparent = True\n                bbox_inches = 'tight', pad_inches = 0.0, dpi = 200)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#  gettting count of peoples\ndef get_count_people (image):\n    # simple preprocessing\n    trans  = transforms.Compose ([transforms,ToTensor (),\n                                 transforms.Normalize ([0.485, 0.456, 0.406],\n                                                      [0.229, 0.224, 0.225])])\n    # sample image with height and width\n    img = Image.open (image).convert ('RGB')\n    height, width = img.size [1], img.size [0]\n    height = round (height / 16) * 16\n    width = round (width / 16) * 16\n    \n    # resize the image\n    img_den = cv2.resize (np.array (img), (width, height), cv2.INTER_CUBIC)\n    \n    # transform \n    img = trans (Image.fromarray (img_den))[None, :]\n    \n    # lets define the model\n    model = M_SFANet_UCF_QNRF.Model ()\n    \n    # load the model\n    model.load_state_dict (torch.load ('/content/best_M-SFANet_UCF_QNRF.pth',\n                                      map_location = torch.device ('cpu')))\n    \n    # evaluating the model\n    model.eval ()\n    dnst_mp = model (img)\n    \n    # final count \n    count = torch.sum (dnst_mp).item ()\n    \n    # return count, density and map\n    return count, img_den, dnst_mp","metadata":{},"execution_count":null,"outputs":[]}]}